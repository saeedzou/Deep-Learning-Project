{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saeedzou/Deep-Learning-Project/blob/dev-saeed/Phase%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toWWvqo7igLV",
        "outputId": "d26f7a9f-2b96-4a9a-8942-0d47701a2326"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MSCTD'...\n",
            "remote: Enumerating objects: 1217, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 1217 (delta 13), reused 7 (delta 3), pack-reused 1190\u001b[K\n",
            "Receiving objects: 100% (1217/1217), 102.24 MiB | 17.46 MiB/s, done.\n",
            "Resolving deltas: 100% (616/616), done.\n",
            "Checking out files: 100% (934/934), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-43sQYxSCsCIxQjOCAS-H4dDI6c2zgi8\n",
            "To: /content/train_ende.zip\n",
            "100% 2.90G/2.90G [00:20<00:00, 140MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k-m84NIuOOTbXjn6ELwj1qBeH7jsN6IO\n",
            "To: /content/dev.zip\n",
            "100% 638M/638M [00:05<00:00, 111MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-0Gg-qpqJpNfLPU7DT81UaFgwu8DVn15\n",
            "To: /content/test.zip\n",
            "100% 641M/641M [00:09<00:00, 70.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# This block downloads and preprocesses the needed data for the project\n",
        "!git clone https://github.com/XL2248/MSCTD\n",
        "!cp MSCTD/MSCTD_data/ende/eng* .\n",
        "!cp MSCTD/MSCTD_data/ende/ima* .\n",
        "!cp MSCTD/MSCTD_data/ende/sent* .\n",
        "!rm -rf MSCTD\n",
        "!pip -q install --upgrade --no-cache-dir gdown\n",
        "!gdown 1-43sQYxSCsCIxQjOCAS-H4dDI6c2zgi8\n",
        "!gdown 1k-m84NIuOOTbXjn6ELwj1qBeH7jsN6IO\n",
        "!gdown 1-0Gg-qpqJpNfLPU7DT81UaFgwu8DVn15\n",
        "!unzip -q train_ende.zip\n",
        "!unzip -q dev.zip\n",
        "!unzip -q test.zip\n",
        "!mv train_ende train\n",
        "!mkdir train/image\n",
        "!mkdir dev/image\n",
        "!mkdir test/image\n",
        "!mv train/*.jpg train/image\n",
        "!mv dev/*.jpg dev/image\n",
        "!mv test/*.jpg test/image\n",
        "!mv *train.txt train\n",
        "!mv *dev.txt dev\n",
        "!mv *test.txt test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.io import read_image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "!pip install facenet-pytorch --quiet\n",
        "from facenet_pytorch import MTCNN\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import shutil\n",
        "import tqdm\n",
        "# ignore np.VisibleDeprecationWarning for image_index because it contains list of lists of different lengths\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) "
      ],
      "metadata": {
        "id": "Fm_Avfwaio3t"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "bMvFPgSuDNL0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSCTD(Dataset):\n",
        "    \"\"\"\n",
        "    :param root: root path of the dataset\n",
        "    :param split: train, dev, test\n",
        "    :param image_transform: transform for image\n",
        "    :param text_transform: transform for text\n",
        "    :param sentiment_transform: transform for sentiment\n",
        "    :param has_data: dict, whether the dataset has image, text\n",
        "    :param text_path: path of the text file\n",
        "    :param image_path: path of the image folder\n",
        "    :param sentiment_path: path of the sentiment file\n",
        "    :param image_index_path: path of the image index file\n",
        "\n",
        "    :return: combination of image, sentiment, text, image_index\n",
        "\n",
        "    Example:\n",
        "    >>> from torchvision import transforms\n",
        "    >>> image_transform = transforms.Compose([\n",
        "    >>>     transforms.Resize((640, 1280)),\n",
        "    >>>     transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
        "    >>> ])\n",
        "    >>> text_transform = None\n",
        "    >>> sentiment_transform = None\n",
        "    >>> dataset = MSCTD(root='data', split='train', image_transform=image_transform,\n",
        "    >>>                 text_transform=text_transform, sentiment_transform=sentiment_transform)\n",
        "    >>> image, text, sentiment = dataset[0]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, root, split, image_transform=None, text_transform=None, sentiment_transform=None,\n",
        "                 has_data={'image': True, 'text': True}, text_path=None, image_path=None, sentiment_path=None,\n",
        "                 image_index_path=None):\n",
        "        data_path = os.path.join(root, split)\n",
        "        default_path = {\n",
        "            'text': os.path.join(data_path, 'english_' + split + '.txt'),\n",
        "            'image': os.path.join(data_path, 'image'),\n",
        "            'sentiment': os.path.join(data_path, 'sentiment_' + split + '.txt'),\n",
        "            'image_index': os.path.join(data_path, 'image_index_' + split + '.txt'),\n",
        "        }\n",
        "        self.image = [] if has_data['image'] else None\n",
        "        self.image_transform = image_transform\n",
        "        self.image_path = image_path if image_path else default_path['image']\n",
        "        self.text = [] if has_data['text'] else None\n",
        "        self.text_transform = text_transform\n",
        "        self.text_path = text_path if text_path else default_path['text']\n",
        "        self.sentiment_path = sentiment_path if sentiment_path else default_path['sentiment']\n",
        "        self.image_index_path = image_index_path if image_index_path else default_path['image_index']\n",
        "        self.sentiment = []\n",
        "        self.image_index = []\n",
        "        self.sentiment_transform = sentiment_transform\n",
        "        self.load_data()\n",
        "        \n",
        "    def load_data(self):\n",
        "        self.sentiment = np.loadtxt(self.sentiment_path, dtype=int)\n",
        "        if self.text is not None:\n",
        "            with open(self.text_path, 'r') as f:\n",
        "                self.text = f.readlines()\n",
        "            self.text = [x.strip() for x in self.text]\n",
        "        with open(self.image_index_path, 'r') as f:\n",
        "            data = f.readlines()\n",
        "        self.image_index = [list(map(int, x[1:-2].split(','))) for x in data]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = None\n",
        "        text = None\n",
        "        sentiment = self.sentiment[index]\n",
        "        if self.image is not None:\n",
        "            imag_path = os.path.join(self.image_path, str(index)+'.jpg')\n",
        "            image = read_image(imag_path)\n",
        "            if self.image_transform:\n",
        "                image = self.image_transform(image)\n",
        "        if self.text is not None:\n",
        "            text = self.text[index]\n",
        "            if self.text_transform:\n",
        "                text = self.text_transform(text)\n",
        "        if self.sentiment_transform:\n",
        "            sentiment = self.sentiment_transform(sentiment)\n",
        "        if text is not None and image is not None:\n",
        "            return image, text, sentiment\n",
        "        elif text is not None:\n",
        "            return text, sentiment\n",
        "        elif image is not None:\n",
        "            return image, sentiment\n",
        "        else:\n",
        "            raise Exception('Either image or text should be not None')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentiment)\n",
        "    "
      ],
      "metadata": {
        "id": "BkBSaR0xix4u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((640, 1280)),\n",
        "    torchvision.transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
        "    ])\n",
        "MSCTD_train = MSCTD(root='.', split='train', image_transform=image_transform, has_data={'image': True, 'text': False})\n",
        "MSCTD_dev = MSCTD(root='.', split='dev', image_transform=image_transform, has_data={'image': True, 'text': False})\n",
        "MSCTD_test = MSCTD(root='.', split='test', image_transform=image_transform, has_data={'image': True, 'text': False})"
      ],
      "metadata": {
        "id": "jreTxJv0i7OQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(MSCTD_train, batch_size=batch_size, shuffle=False)\n",
        "dev_loader = DataLoader(MSCTD_dev, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(MSCTD_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Hnejgp2fi_U7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1 : Face Analysis"
      ],
      "metadata": {
        "id": "53xI-xVKM0EH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subpart 1"
      ],
      "metadata": {
        "id": "_NwwmluwNB-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1"
      ],
      "metadata": {
        "id": "uz5tVFHeNIF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = torchvision.transforms.Compose([\n",
        "#     torchvision.transforms.Lambda(lambda x: x.permute(2, 0, 1)),\n",
        "#     torchvision.transforms.Resize((160, 160)),\n",
        "#     torchvision.transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
        "#     ])"
      ],
      "metadata": {
        "id": "fz1T9DHVrGRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function like crop_faces that takes as input an image, and the boxes\n",
        "# and returns a list with length of batch size, where each element is a list of the cropped faces for that image and none if no faces are detected\n",
        "def crop_faces_batch(image, boxes):\n",
        "    \"\"\"\n",
        "    :param image: image tensor of shape (batch_size, height, width, channels)\n",
        "    :param boxes: bounding boxes for the faces of shape (batch_size, num_faces, 4) where 4 is (x1, y1, x2, y2). Note that some images may not have any faces\n",
        "    :return: \n",
        "    :param cropped_images: list of cropped faces with length equal to the batch size. each element is a list of cropped faces for that image and none if no faces are detected\n",
        "    \"\"\"\n",
        "    cropped_images = []\n",
        "    for i in range(image.shape[0]):\n",
        "        current_image = image[i]\n",
        "        if boxes[i] is not None:\n",
        "            current_cropped_images = []\n",
        "            for j in range(boxes[i].shape[0]):\n",
        "                current_box = boxes[i][j]\n",
        "                x1, y1, x2, y2 = current_box\n",
        "                x1, y1, x2, y2 = max(int(x1), 0), max(int(y1), 0), max(int(x2), 0), max(int(y2), 0)\n",
        "                cropped_image = current_image[y1:y2, x1:x2]\n",
        "                current_cropped_images.append(cropped_image)\n",
        "            cropped_images.append(current_cropped_images)\n",
        "        else:\n",
        "            cropped_images.append(None)\n",
        "    return cropped_images"
      ],
      "metadata": {
        "id": "zp6VVhwXdHtR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that given the data loader, uses the crop_faces_batch to save the cropped images and their sentiments\n",
        "def save_cropped_images(data_loader, face_detector, faces_path, sentiments_path):\n",
        "    \"\"\"\n",
        "    :param data_loader: data loader for the dataset\n",
        "    :param face_detector: face detector\n",
        "    :param faces_path: path to save the cropped faces\n",
        "    :param sentiments_path: path to save the sentiments\n",
        "    :return: \n",
        "    \"\"\"\n",
        "    sentiments = []\n",
        "    image_index = 0\n",
        "    if not os.path.exists(faces_path):\n",
        "        os.makedirs(faces_path)\n",
        "    else:\n",
        "        shutil.rmtree(faces_path)\n",
        "        os.makedirs(faces_path)\n",
        "    for i, (image, sentiment) in enumerate(data_loader):\n",
        "        if i % 50 == 0:\n",
        "              print(\"Processing batch {}/{}\".format(i+1, len(data_loader)))\n",
        "        boxes, _ = face_detector.detect(image)\n",
        "        cropped_images_batch = crop_faces_batch(image, boxes)\n",
        "        # cropped_images_batch contains a list, where each element is a list with length equal to the number of faces in that image and none if no faces are detected\n",
        "        # we want to save the cropped images with their corresponding sentiments\n",
        "        for j in range(len(cropped_images_batch)):\n",
        "            if cropped_images_batch[j] is not None:\n",
        "                sentiments.extend([sentiment[j]] * len(cropped_images_batch[j]))\n",
        "                for k in range(len(cropped_images_batch[j])):\n",
        "                    image_path = os.path.join(faces_path, str(image_index)+'.jpg')\n",
        "                    image = cropped_images_batch[j][k].numpy()\n",
        "                    image = Image.fromarray(image)\n",
        "                    image.save(image_path)\n",
        "                    image_index += 1\n",
        "    with open(sentiments_path, 'w') as f:\n",
        "        for sentiment in sentiments:\n",
        "            f.write(str(sentiment.item()) + '\\n')\n",
        "        \n",
        "    \n",
        "face_detector = MTCNN(keep_all=True, device=device)\n",
        "# save the cropped faces and sentiments for the train, dev, and test sets\n",
        "save_cropped_images(train_loader, face_detector, 'data/train/image', 'data/train/train_sentiments.txt')\n",
        "save_cropped_images(dev_loader, face_detector, 'data/dev/image', 'data/dev/dev_sentiments.txt')\n",
        "save_cropped_images(test_loader, face_detector, 'data/test/image', 'data/test/test_sentiments.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZc4M3GUgLV1",
        "outputId": "35c98dbf-92ef-4e86-d53e-59f5f85ea1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1/633\n",
            "Processing batch 51/633\n",
            "Processing batch 101/633\n",
            "Processing batch 151/633\n",
            "Processing batch 201/633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2"
      ],
      "metadata": {
        "id": "uU8PY8npNLzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a custom dataset class that takes as input the cropped faces path and the sentiments path \n",
        "# the sentiments are the targets for the model\n",
        "class CroppedFaces(Dataset):\n",
        "    def __init__(self, faces_path, sentiments_path, transform=None):\n",
        "        # TODO: implement this function\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # TODO: implement this function\n",
        "        raise NotImplementedError\n",
        "    def __len__(self):\n",
        "        # TODO: implement this function\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "LxaAKNHZKM6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create instances of the custom dataset class for the train, dev, and test sets\n",
        "# define the data loaders for the train, dev, and test sets\n",
        "# define the image transform for the train, dev, and test sets\n",
        "image_transform = None # TODO\n",
        "\n",
        "train_dataset = None # TODO\n",
        "dev_dataset = None # TODO\n",
        "test_dataset = None # TODO\n",
        "\n",
        "train_loader = None # TODO\n",
        "dev_loader = None # TODO\n",
        "test_loader = None # TODO"
      ],
      "metadata": {
        "id": "DAyusavkMpc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "class FaceSentimentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: implement this function\n",
        "        raise NotImplementedError\n",
        "    def forward(self, x):\n",
        "        # TODO: implement this function\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "KF_x-Wv5Mq1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, move it to device, and define the optimizer and loss function\n",
        "model = None # TODO\n",
        "model = model.to(device)\n",
        "optimizer = None # TODO\n",
        "loss_function = None # TODO"
      ],
      "metadata": {
        "id": "a5yTuZ2MMsBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to train the model\n",
        "def train(model, train_loader, dev_loader, optimizer, loss_function, num_epochs=10):\n",
        "    \"\"\"\n",
        "    :param model: model to train\n",
        "    :param train_loader: train data loader\n",
        "    :param dev_loader: dev data loader\n",
        "    :param optimizer: optimizer to use\n",
        "    :param loss_function: loss function to use\n",
        "    :param num_epochs: number of epochs to train for\n",
        "    :return: train_losses, train_accs, dev_losses, dev_accs\n",
        "    \"\"\"\n",
        "    # TODO: implement this function\n",
        "    raise NotImplementedError\n"
      ],
      "metadata": {
        "id": "J5PapzdmMtJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "train_losses, train_accs, dev_losses, dev_accs = train(model, train_loader, dev_loader, optimizer, loss_function, num_epochs=10)"
      ],
      "metadata": {
        "id": "Lu45LfXVMuTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that given the train_losses, train_accs, dev_losses, and dev_accs, plots the train and dev losses and accuracies\n",
        "def plot_train_dev(train_losses, train_accs, dev_losses, dev_accs):\n",
        "    # TODO: implement this function\n",
        "    raise NotImplementedError\n",
        "\n",
        "# plot the train and dev losses and accuracies\n",
        "plot_train_dev(train_losses, train_accs, dev_losses, dev_accs)"
      ],
      "metadata": {
        "id": "IeAQJmLaMvTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3"
      ],
      "metadata": {
        "id": "fjDOQ9jONRHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that takes as input the FaceSentimentClassifier model, the crop_faces_batch function, the data loader, and the device, the face_detector\n",
        "# the function should iterate over the data loader and for each image, detect the faces in the image using the face_detector\n",
        "# Then pass the image and boxes to the crop_faces_batch function to get the cropped faces\n",
        "# Then iterate over the cropped faces list\n",
        "# For each image, the corresponding element in the cropped_faces list is a list of cropped faces\n",
        "# If the element is not None, then iterate over the list and pass each cropped face to the model to get the sentiment prediction\n",
        "# Then combine the predictions of the cropped faces in the image to get the sentiment prediction for the image\n",
        "# Finally, return the predictions and the targets\n",
        "def get_predictions(model, crop_faces_batch, data_loader, device, face_detector):\n",
        "    # TODO: implement this function\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "eakzSsDzNSko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "m92ONi1-jr__"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1UFBVxBjubQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}