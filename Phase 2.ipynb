{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saeedzou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saeedzou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\saeedzou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saeedzou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import nltk.tokenize as tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSCTD(Dataset):\n",
    "    \"\"\"\n",
    "    :param root: root path of the dataset\n",
    "    :param split: train, dev, test\n",
    "    :param image_transform: transform for image\n",
    "    :param text_transform: transform for text\n",
    "    :param sentiment_transform: transform for sentiment\n",
    "    :param has_data: dict, whether the dataset has image, text\n",
    "    :param text_path: path of the text file\n",
    "    :param image_path: path of the image folder\n",
    "    :param sentiment_path: path of the sentiment file\n",
    "    :param image_index_path: path of the image index file\n",
    "\n",
    "    :return: combination of image, sentiment, text, image_index\n",
    "\n",
    "    Example:\n",
    "    >>> from torchvision import transforms\n",
    "    >>> image_transform = transforms.Compose([\n",
    "    >>>     transforms.Resize((640, 1280)),\n",
    "    >>>     transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
    "    >>> ])\n",
    "    >>> text_transform = None\n",
    "    >>> sentiment_transform = None\n",
    "    >>> dataset = MSCTD(root='data', split='train', image_transform=image_transform,\n",
    "    >>>                 text_transform=text_transform, sentiment_transform=sentiment_transform)\n",
    "    >>> image, text, sentiment = dataset[0]\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, root, split, image_transform=None, text_transform=None, sentiment_transform=None,\n",
    "                 has_data={'image': True, 'text': True}, text_path=None, image_path=None, sentiment_path=None,\n",
    "                 image_index_path=None):\n",
    "        data_path = os.path.join(root, split)\n",
    "        default_path = {\n",
    "            'text': os.path.join(data_path, 'english_' + split + '.txt'),\n",
    "            'image': os.path.join(data_path, 'image'),\n",
    "            'sentiment': os.path.join(data_path, 'sentiment_' + split + '.txt'),\n",
    "            'image_index': os.path.join(data_path, 'image_index_' + split + '.txt'),\n",
    "        }\n",
    "        self.image = [] if has_data['image'] else None\n",
    "        self.image_transform = image_transform\n",
    "        self.image_path = image_path if image_path else default_path['image']\n",
    "        self.text = [] if has_data['text'] else None\n",
    "        self.text_transform = text_transform\n",
    "        self.text_path = text_path if text_path else default_path['text']\n",
    "        self.sentiment_path = sentiment_path if sentiment_path else default_path['sentiment']\n",
    "        self.image_index_path = image_index_path if image_index_path else default_path['image_index']\n",
    "        self.sentiment = []\n",
    "        self.image_index = []\n",
    "        self.sentiment_transform = sentiment_transform\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.sentiment = np.loadtxt(self.sentiment_path, dtype=int)\n",
    "        if self.text is not None:\n",
    "            with open(self.text_path, 'r') as f:\n",
    "                self.text = f.readlines()\n",
    "            self.text = [x.strip() for x in self.text]\n",
    "        with open(self.image_index_path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "        self.image_index = [list(map(int, x[1:-2].split(','))) for x in data]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = None\n",
    "        text = None\n",
    "        sentiment = self.sentiment[index]\n",
    "        if self.image is not None:\n",
    "            imag_path = os.path.join(self.image_path, str(index)+'.jpg')\n",
    "            image = read_image(imag_path)\n",
    "            if self.image_transform:\n",
    "                image = self.image_transform(image)\n",
    "        if self.text is not None:\n",
    "            text = self.text[index]\n",
    "            if self.text_transform:\n",
    "                text = self.text_transform(text)\n",
    "        if self.sentiment_transform:\n",
    "            sentiment = self.sentiment_transform(sentiment)\n",
    "        if text is not None and image is not None:\n",
    "            return image, text, sentiment\n",
    "        elif text is not None:\n",
    "            return text, sentiment\n",
    "        elif image is not None:\n",
    "            return image, sentiment\n",
    "        else:\n",
    "            raise Exception('Either image or text should be not None')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSCTD_train = MSCTD(root='./data', split='train', has_data={'image': False, 'text': True})\n",
    "MSCTD_dev = MSCTD(root='./data', split='dev', has_data={'image': False, 'text': True})\n",
    "MSCTD_test = MSCTD(root='./data', split='test', has_data={'image': False, 'text': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        # preprocess text\n",
    "        self.dataset.text = [self.preprocess_text(text) for text in self.dataset.text]\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "            # Replace contractions with original text\n",
    "        text = re.sub(r'(\\b[Aa]in\\'t\\b)', \"am not\", text)\n",
    "        text = re.sub(r'(\\b[Hh]a\\'t\\b)', \"has\", text)\n",
    "        text = re.sub(r'(\\b[Ii]\\'m\\b)', \"i am\", text)\n",
    "        # convert 's to is\n",
    "        text = re.sub(r'(\\b\\'s\\b)', \" is\", text)\n",
    "        text = re.sub(r'(\\b\\'re\\b)', \" are\", text)\n",
    "        text = re.sub(r'(\\b\\'ve\\b)', \" have\", text)\n",
    "        text = re.sub(r'(\\b\\'d\\b)', \" would\", text)\n",
    "        text = re.sub(r'(\\b\\'ll\\b)', \" will\", text)\n",
    "        text = re.sub(r'(\\b[Ss]han\\'t\\b)', \"shall not\", text)\n",
    "        text = re.sub(r'(\\b[Ww]on\\'t\\b)', \"will not\", text)\n",
    "        text = re.sub(r'(\\b[Ww]ouldn\\'t\\b)', \"would not\", text)\n",
    "        text = re.sub(r'(\\b[Dd]on\\'t\\b)', \"do not\", text)\n",
    "        text = re.sub(r'(\\b[Cc]an\\'t\\b)', \"can not\", text)\n",
    "        text = re.sub(r'(\\b[Ii]s\\'nt\\b)', \"is not\", text)\n",
    "        text = re.sub(r'(\\b[Ww]eren\\'t\\b)', \"were not\", text)\n",
    "        text = re.sub(r'(\\b[Hh]aven\\'t\\b)', \"have not\", text)\n",
    "        text = re.sub(r'(\\b[Hh]adn\\'t\\b)', \"had not\", text)\n",
    "        text = re.sub(r'(\\b[Hh]asn\\'t\\b)', \"has not\", text)\n",
    "        text = re.sub(r'(\\b[Hh]adn\\'t\\b)', \"had not\", text)\n",
    "        text = re.sub(r'(\\b[Dd]idn\\'t\\b)', \"did not\", text)\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenize text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Lemmatize tokens\n",
    "        tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the caption and sentiment from the dataset\n",
    "        caption, sentiment = self.dataset[index]\n",
    "\n",
    "        # Preprocess the caption\n",
    "        # preprocessed_caption = self.preprocess_text(caption)\n",
    "        \n",
    "        return caption, sentiment\n",
    "\n",
    "# Test the TextPreprocessor\n",
    "text_preprocessor = TextPreprocessor(MSCTD_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I mean, that's not what I want.\", 1) (['mean', 'want'], 1)\n"
     ]
    }
   ],
   "source": [
    "i = 2000\n",
    "print(MSCTD_train[i], text_preprocessor[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\saeedzou\\AppData\\Local\\Programs\\Python\\Python39\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 21] The device is not ready: 'D:\\\\nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Fit the vocabulary\u001b[39;00m\n\u001b[0;32m     24\u001b[0m vocabulary \u001b[39m=\u001b[39m Vocabulary()\n\u001b[1;32m---> 25\u001b[0m vocabulary\u001b[39m.\u001b[39mfit([text \u001b[39mfor\u001b[39;00m text, _ \u001b[39min\u001b[39;00m text_preprocessor])\n",
      "Cell \u001b[1;32mIn[85], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Fit the vocabulary\u001b[39;00m\n\u001b[0;32m     24\u001b[0m vocabulary \u001b[39m=\u001b[39m Vocabulary()\n\u001b[1;32m---> 25\u001b[0m vocabulary\u001b[39m.\u001b[39mfit([text \u001b[39mfor\u001b[39;00m text, _ \u001b[39min\u001b[39;00m text_preprocessor])\n",
      "Cell \u001b[1;32mIn[73], line 61\u001b[0m, in \u001b[0;36mTextPreprocessor.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     58\u001b[0m caption, sentiment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[index]\n\u001b[0;32m     60\u001b[0m \u001b[39m# Preprocess the caption\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m preprocessed_caption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess_text(caption)\n\u001b[0;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m preprocessed_caption, sentiment\n",
      "Cell \u001b[1;32mIn[73], line 52\u001b[0m, in \u001b[0;36mTextPreprocessor.preprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     49\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     51\u001b[0m \u001b[39m# Lemmatize tokens\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m tokens \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(token, get_wordnet_pos(token)) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens]\n\u001b[0;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m tokens\n",
      "Cell \u001b[1;32mIn[73], line 52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     51\u001b[0m \u001b[39m# Lemmatize tokens\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m tokens \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(token, get_wordnet_pos(token)) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens]\n\u001b[0;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m tokens\n",
      "Cell \u001b[1;32mIn[73], line 3\u001b[0m, in \u001b[0;36mget_wordnet_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_wordnet_pos\u001b[39m(word):\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     tag \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mpos_tag([word])[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mupper()\n\u001b[0;32m      4\u001b[0m     tag_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mJ\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mADJ,\n\u001b[0;32m      5\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mN\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mNOUN,\n\u001b[0;32m      6\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mVERB,\n\u001b[0;32m      7\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m: wordnet\u001b[39m.\u001b[39mADV}\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m tag_dict\u001b[39m.\u001b[39mget(tag, wordnet\u001b[39m.\u001b[39mNOUN)\n",
      "File \u001b[1;32mc:\\Users\\saeedzou\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\__init__.py:133\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meng\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    109\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    134\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger)\n",
      "File \u001b[1;32mc:\\Users\\saeedzou\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\__init__.py:97\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m     95\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[0;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\saeedzou\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\tag\\perceptron.py:140\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[1;32m--> 140\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(find(\u001b[39m'\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mPICKLE))\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mc:\\Users\\saeedzou\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\nltk\\data.py:621\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39m# Check each item in our path\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[39mfor\u001b[39;00m path_ \u001b[39min\u001b[39;00m paths:\n\u001b[0;32m    620\u001b[0m     \u001b[39m# Is the path item a zipfile?\u001b[39;00m\n\u001b[1;32m--> 621\u001b[0m     \u001b[39mif\u001b[39;00m path_ \u001b[39mand\u001b[39;00m (os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misfile(path_) \u001b[39mand\u001b[39;00m path_\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.zip\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m    622\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m             \u001b[39mreturn\u001b[39;00m ZipFilePathPointer(path_, resource_name)\n",
      "File \u001b[1;32mc:\\Users\\saeedzou\\AppData\\Local\\Programs\\Python\\Python39\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     31\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.counter = Counter()\n",
    "        self.total_words = 0\n",
    "\n",
    "    def fit(self, texts):\n",
    "        for text in texts:\n",
    "            self.counter.update(text.split())\n",
    "        \n",
    "        for word, count in self.counter.items():\n",
    "            self.word2idx[word] = len(self.idx2word)\n",
    "            self.idx2word.append(word)\n",
    "            self.total_words += count\n",
    "\n",
    "    def transform(self, texts):\n",
    "        text_indices = []\n",
    "        for text in texts:\n",
    "            text_indices.append([self.word2idx[word] for word in text.split() if word in self.word2idx])\n",
    "        return text_indices\n",
    "\n",
    "# Fit the vocabulary\n",
    "vocabulary = Vocabulary()\n",
    "vocabulary.fit([text for text, _ in text_preprocessor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fbe9694f7587329a2893969593bb646d9caf203732995a36644052b7dd475e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
