{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saeedzou/Deep-Learning-Project/blob/dev-saeed/Phase%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This block downloads and preprocesses the needed data for the project\n",
        "!git clone https://github.com/XL2248/MSCTD\n",
        "!cp MSCTD/MSCTD_data/ende/eng* .\n",
        "!cp MSCTD/MSCTD_data/ende/ima* .\n",
        "!cp MSCTD/MSCTD_data/ende/sent* .\n",
        "!rm -rf MSCTD\n",
        "!pip -q install --upgrade --no-cache-dir gdown\n",
        "!gdown 1-43sQYxSCsCIxQjOCAS-H4dDI6c2zgi8\n",
        "!gdown 1k-m84NIuOOTbXjn6ELwj1qBeH7jsN6IO\n",
        "!gdown 1-0Gg-qpqJpNfLPU7DT81UaFgwu8DVn15\n",
        "!unzip -q train_ende.zip\n",
        "!unzip -q dev.zip\n",
        "!unzip -q test.zip\n",
        "!mv train_ende train\n",
        "!mkdir train/image\n",
        "!mkdir dev/image\n",
        "!mkdir test/image\n",
        "!mv train/*.jpg train/image\n",
        "!mv dev/*.jpg dev/image\n",
        "!mv test/*.jpg test/image\n",
        "!mv *train.txt train\n",
        "!mv *dev.txt dev\n",
        "!mv *test.txt test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efwLVBYQo76z",
        "outputId": "df2f17a3-4b4d-4d0a-e907-8a4956ba2bfe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MSCTD'...\n",
            "remote: Enumerating objects: 1217, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 1217 (delta 13), reused 7 (delta 3), pack-reused 1190\u001b[K\n",
            "Receiving objects: 100% (1217/1217), 102.24 MiB | 12.73 MiB/s, done.\n",
            "Resolving deltas: 100% (616/616), done.\n",
            "Updating files: 100% (934/934), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-43sQYxSCsCIxQjOCAS-H4dDI6c2zgi8\n",
            "To: /content/train_ende.zip\n",
            "100% 2.90G/2.90G [00:11<00:00, 258MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k-m84NIuOOTbXjn6ELwj1qBeH7jsN6IO\n",
            "To: /content/dev.zip\n",
            "100% 638M/638M [00:02<00:00, 269MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-0Gg-qpqJpNfLPU7DT81UaFgwu8DVn15\n",
            "To: /content/test.zip\n",
            "100% 641M/641M [00:02<00:00, 294MB/s]\n",
            "mkdir: cannot create directory ‘train/image’: File exists\n",
            "mkdir: cannot create directory ‘dev/image’: File exists\n",
            "mkdir: cannot create directory ‘test/image’: File exists\n",
            "mv: cannot stat 'train/*.jpg': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM987Bi4o2q2",
        "outputId": "585bf661-94a4-4b7a-9761-d428d6606bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import nltk.tokenize as tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "import gc\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install -q transformers\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MSCTD(Dataset):\n",
        "    \"\"\"\n",
        "    :param root: root path of the dataset\n",
        "    :param split: train, dev, test\n",
        "    :param image_transform: transform for image\n",
        "    :param text_transform: transform for text\n",
        "    :param sentiment_transform: transform for sentiment\n",
        "    :param has_data: dict, whether the dataset has image, text\n",
        "    :param text_path: path of the text file\n",
        "    :param image_path: path of the image folder\n",
        "    :param sentiment_path: path of the sentiment file\n",
        "    :param image_index_path: path of the image index file\n",
        "\n",
        "    :return: combination of image, sentiment, text, image_index\n",
        "\n",
        "    Example:\n",
        "    >>> from torchvision import transforms\n",
        "    >>> image_transform = transforms.Compose([\n",
        "    >>>     transforms.Resize((640, 1280)),\n",
        "    >>>     transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
        "    >>> ])\n",
        "    >>> text_transform = None\n",
        "    >>> sentiment_transform = None\n",
        "    >>> dataset = MSCTD(root='data', split='train', image_transform=image_transform,\n",
        "    >>>                 text_transform=text_transform, sentiment_transform=sentiment_transform)\n",
        "    >>> image, text, sentiment = dataset[0]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, root, split, image_transform=None, text_transform=None, sentiment_transform=None,\n",
        "                 has_data={'image': True, 'text': True}, text_path=None, image_path=None, sentiment_path=None,\n",
        "                 image_index_path=None):\n",
        "        data_path = os.path.join(root, split)\n",
        "        default_path = {\n",
        "            'text': os.path.join(data_path, 'english_' + split + '.txt'),\n",
        "            'image': os.path.join(data_path, 'image'),\n",
        "            'sentiment': os.path.join(data_path, 'sentiment_' + split + '.txt'),\n",
        "            'image_index': os.path.join(data_path, 'image_index_' + split + '.txt'),\n",
        "        }\n",
        "        self.image = [] if has_data['image'] else None\n",
        "        self.image_transform = image_transform\n",
        "        self.image_path = image_path if image_path else default_path['image']\n",
        "        self.text = [] if has_data['text'] else None\n",
        "        self.text_transform = text_transform\n",
        "        self.text_path = text_path if text_path else default_path['text']\n",
        "        self.sentiment_path = sentiment_path if sentiment_path else default_path['sentiment']\n",
        "        self.image_index_path = image_index_path if image_index_path else default_path['image_index']\n",
        "        self.sentiment = []\n",
        "        self.image_index = []\n",
        "        self.sentiment_transform = sentiment_transform\n",
        "        self.load_data()\n",
        "        \n",
        "    def load_data(self):\n",
        "        self.sentiment = np.loadtxt(self.sentiment_path, dtype=int)\n",
        "        if self.text is not None:\n",
        "            with open(self.text_path, 'r') as f:\n",
        "                self.text = f.readlines()\n",
        "            self.text = [x.strip() for x in self.text]\n",
        "        with open(self.image_index_path, 'r') as f:\n",
        "            data = f.readlines()\n",
        "        self.image_index = [list(map(int, x[1:-2].split(','))) for x in data]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = None\n",
        "        text = None\n",
        "        sentiment = self.sentiment[index]\n",
        "        if self.image is not None:\n",
        "            imag_path = os.path.join(self.image_path, str(index)+'.jpg')\n",
        "            image = read_image(imag_path)\n",
        "            if self.image_transform:\n",
        "                image = self.image_transform(image)\n",
        "        if self.text is not None:\n",
        "            text = self.text[index]\n",
        "            if self.text_transform:\n",
        "                text = self.text_transform(text)\n",
        "        if self.sentiment_transform:\n",
        "            sentiment = self.sentiment_transform(sentiment)\n",
        "        if text is not None and image is not None:\n",
        "            return image, text, sentiment\n",
        "        elif text is not None:\n",
        "            return text, sentiment\n",
        "        elif image is not None:\n",
        "            return image, sentiment\n",
        "        else:\n",
        "            raise Exception('Either image or text should be not None')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentiment)"
      ],
      "metadata": {
        "id": "1HI_093VpCl1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToPILImage(),\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "MSCTD_train = MSCTD(root='.', split='train', image_transform=image_transform, has_data={'image': True, 'text': True})\n",
        "MSCTD_dev = MSCTD(root='.', split='dev', image_transform=image_transform, has_data={'image': True, 'text': True})\n",
        "MSCTD_test = MSCTD(root='.', split='test', image_transform=image_transform, has_data={'image': True, 'text': True})"
      ],
      "metadata": {
        "id": "OQr-lM0Epwx7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def text_preprocessor(dataset):\n",
        "      # preprocess text\n",
        "      dataset.text = [preprocess_text(text) for text in dataset.text]\n",
        "\n",
        "def preprocess_text(text):\n",
        "      # Replace contractions with original text\n",
        "  text = re.sub(r'(\\b[Aa]in\\'t\\b)', \"am not\", text)\n",
        "  text = re.sub(r'(\\b[Hh]a\\'t\\b)', \"has\", text)\n",
        "  text = re.sub(r'(\\b[Ii]\\'m\\b)', \"i am\", text)\n",
        "  text = re.sub(r'(\\b\\'s\\b)', \" is\", text)\n",
        "  text = re.sub(r'(\\b\\'re\\b)', \" are\", text)\n",
        "  text = re.sub(r'(\\b\\'ve\\b)', \" have\", text)\n",
        "  text = re.sub(r'(\\b\\'d\\b)', \" would\", text)\n",
        "  text = re.sub(r'(\\b\\'ll\\b)', \" will\", text)\n",
        "  text = re.sub(r'(\\b[Ss]han\\'t\\b)', \"shall not\", text)\n",
        "  text = re.sub(r'(\\b[Ww]on\\'t\\b)', \"will not\", text)\n",
        "  text = re.sub(r'(\\b[Ww]ouldn\\'t\\b)', \"would not\", text)\n",
        "  text = re.sub(r'(\\b[Dd]on\\'t\\b)', \"do not\", text)\n",
        "  text = re.sub(r'(\\b[Cc]an\\'t\\b)', \"can not\", text)\n",
        "  text = re.sub(r'(\\b[Ii]s\\'nt\\b)', \"is not\", text)\n",
        "  text = re.sub(r'(\\b[Ww]eren\\'t\\b)', \"were not\", text)\n",
        "  text = re.sub(r'(\\b[Hh]aven\\'t\\b)', \"have not\", text)\n",
        "  text = re.sub(r'(\\b[Hh]adn\\'t\\b)', \"had not\", text)\n",
        "  text = re.sub(r'(\\b[Hh]asn\\'t\\b)', \"has not\", text)\n",
        "  text = re.sub(r'(\\b[Hh]adn\\'t\\b)', \"had not\", text)\n",
        "  text = re.sub(r'(\\b[Dd]idn\\'t\\b)', \"did not\", text)\n",
        "\n",
        "  # Remove punctuation\n",
        "  text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "  # Convert to lowercase\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove stop words\n",
        "  # tokens = [token for token in tokens if token not in stop_words]\n",
        "  # Remove numbers\n",
        "  tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "  # Lemmatize tokens\n",
        "  tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
        "\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# Test the TextPreprocessor\n",
        "text_preprocessor(MSCTD_train)\n",
        "text_preprocessor(MSCTD_dev)\n",
        "text_preprocessor(MSCTD_test)"
      ],
      "metadata": {
        "id": "SklafdvXpxGn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loaders for the train, dev, and test sets from MSCTD\n",
        "batch_size = 32\n",
        "train_loader_MSCTD = DataLoader(MSCTD_train, batch_size=batch_size, shuffle=True)\n",
        "dev_loader_MSCTD = DataLoader(MSCTD_dev, batch_size=batch_size, shuffle=False)\n",
        "test_loader_MSCTD = DataLoader(MSCTD_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "L2o3_ZikrHqM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm\n",
        "import timm"
      ],
      "metadata": {
        "id": "5DB5IKOAsOro"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a image_encoder model using resnet50 on timm\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        self.resnet = timm.create_model('resnet50', pretrained=pretrained)\n",
        "        self.resnet.fc = nn.Identity()\n",
        "        # freeze the model\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# define a text_encoder model using bert-base-uncased on huggingface (note that the dataset is not tokenized)\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, bert_model_name, tokenizer):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.tokenizer = tokenizer\n",
        "        # freeze the model\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # tokenize the text and pad it to the max length=25\n",
        "        x = self.tokenizer(x, padding=True, truncation=True, max_length=25, return_tensors='pt')\n",
        "        # get the output of the bert model\n",
        "        output = self.bert(input_ids=x['input_ids'], attention_mask=x['attention_mask'])\n",
        "        # get the last hidden state of the [CLS] token\n",
        "        return output.last_hidden_state[:, 0, :]"
      ],
      "metadata": {
        "id": "ZFF4CmI7rIFY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "KHaYQq-2VvF0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define image encoder, tokenizer, text encoder\n",
        "image_encoder = ImageEncoder().to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text_encoder = TextEncoder('bert-base-uncased', tokenizer)"
      ],
      "metadata": {
        "id": "vjXrwwkxVoU4",
        "outputId": "d4022c47-5985-4113-a948-7abfe976087a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# define the classifier\n",
        "classifier = MLPClassifier(2048+768, 512, 3).to(device)"
      ],
      "metadata": {
        "id": "nyy4gYzmVq71"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "NDaOra1zVviw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "train_accs = []\n",
        "dev_accs = []\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # train\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    for i, (images, texts, labels) in enumerate(train_loader_MSCTD):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # get the image and text embeddings\n",
        "        image_embeddings = image_encoder(images)\n",
        "        text_embeddings = text_encoder(texts)\n",
        "        text_embeddings = text_embeddings.to(device)\n",
        "        # concatenate the image and text embeddings\n",
        "        embeddings = torch.cat((image_embeddings, text_embeddings), dim=1)\n",
        "        # get the predictions\n",
        "        predictions = classifier(embeddings)\n",
        "        # calculate the loss\n",
        "        loss = F.cross_entropy(predictions, labels)\n",
        "        # calculate the accuracy\n",
        "        acc = (predictions.argmax(dim=1) == labels).float().mean()\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # update the loss and accuracy\n",
        "        train_loss += loss.item()\n",
        "        train_acc += acc.item()\n",
        "    # calculate the average loss and accuracy\n",
        "    train_loss /= len(train_loader_MSCTD)\n",
        "    train_acc /= len(train_loader_MSCTD)\n",
        "    # save the loss and accuracy\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    # evaluate\n",
        "    dev_loss = 0\n",
        "    dev_acc = 0\n",
        "    for i, (images, texts, labels) in enumerate(dev_loader_MSCTD):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # get the image and text embeddings\n",
        "        image_embeddings = image_encoder(images)\n",
        "        text_embeddings = text_encoder(texts)\n",
        "        text_embeddings = text_embeddings.to(device)\n",
        "        # concatenate the image and text embeddings\n",
        "        embeddings = torch.cat((image_embeddings, text_embeddings), dim=1)\n",
        "        # get the predictions\n",
        "        predictions = classifier(embeddings)\n",
        "        # calculate the loss\n",
        "        loss = F.cross_entropy(predictions, labels)\n",
        "        # calculate the accuracy\n",
        "        acc = (predictions.argmax(dim=1) == labels).float().mean()\n",
        "        # update the loss and accuracy\n",
        "        dev_loss += loss.item()\n",
        "        dev_acc += acc.item()\n",
        "    # calculate the average loss and accuracy\n",
        "    dev_loss /= len(dev_loader_MSCTD)\n",
        "    dev_acc /= len(dev_loader_MSCTD)\n",
        "    # save the loss and accuracy\n",
        "    dev_losses.append(dev_loss)\n",
        "    dev_accs.append(dev_acc)\n",
        "    # print the loss and accuracy\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Dev Loss: {dev_loss:.4f}, Dev Acc: {dev_acc:.4f}')"
      ],
      "metadata": {
        "id": "DQybwz-iVy5v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e2f775898ea1037b7c9eeca230f0f6a000b04cbdfbf9c82813024cd1951cbec6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}